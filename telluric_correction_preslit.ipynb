{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ac33e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:20:23.412391Z",
     "start_time": "2023-01-18T20:20:23.408473Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "from astropy.table import Table\n",
    "import scipy.signal as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee628ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:20:23.941869Z",
     "start_time": "2023-01-18T20:20:23.930685Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a funtion to return 0 when divided by zero\n",
    "def div0( a, b, fill=0. ):\n",
    "    \"\"\" a / b, divide by 0 -> `fill`\n",
    "        div0( [-1, 0, 1], 0, fill=np.nan) -> [nan nan nan]\n",
    "        div0( 1, 0, fill=np.inf ) -> inf\n",
    "    \"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        c = np.true_divide( a, b )\n",
    "    if np.isscalar( c ):\n",
    "        return c if np.isfinite( c ) \\\n",
    "            else fill\n",
    "    else:\n",
    "        c[ ~ np.isfinite( c )] = fill\n",
    "        return c\n",
    "\n",
    "def normalize(data):\n",
    "    return ((data - np.min(data)) / (np.max(data) - np.min(data))) + 1\n",
    "\n",
    "\n",
    "def df2array(df):\n",
    "    lists = []\n",
    "    for i in range(len(df)):\n",
    "        lists.append(df.iloc[i][0])\n",
    "    return np.array(lists)\n",
    "\n",
    "\n",
    "# define a function that resamples the data based on wavelength\n",
    "# has 3 1darray inputs: target_array is the array that has the sampling you want;\n",
    "# ref_array is the array that has the same kind of data with the target_array but with different sampling rate:\n",
    "# and the input_array is the array sharing the same sampling rate but not necessicirally the same data with the ref_array\n",
    "\n",
    "# this function resamlpes the ref_array to the same sampling as the target_array first and then\n",
    "# use the indecies to resample the input array, output is the resampled input_array\n",
    "\n",
    "\n",
    "def resampling(target_array, ref_array, input_array, resamp_ref_array=False):\n",
    "\n",
    "    # for each value in the target_array, find the indeices of the value(s)\n",
    "    # nearest to it in the ref_array, output has the same legth as the target_array\n",
    "    index = abs(target_array[:, None] - ref_array[None, :]).argmin(axis=-1)\n",
    "\n",
    "    # based on the indices find the corresponding values in the input_array and\n",
    "    # write everything into a new array\n",
    "    resampled_array = []\n",
    "    for i in index:\n",
    "        resampled_array.append(input_array[i])\n",
    "\n",
    "    # choose if to output the resampled ref_array, sometimes you need this\n",
    "    if resamp_ref_array == True:\n",
    "        resampled_ref_array = []\n",
    "        for i in index:\n",
    "            resampled_ref_array.append(ref_array[i])\n",
    "        return resampled_array, resampled_ref_array\n",
    "    else:\n",
    "        return resampled_array\n",
    "\n",
    "\n",
    "def find_files(date, name):\n",
    "\n",
    "    spec1d_list = glob.glob('NIRES_PypeIt/' + date + '_reduced' + '/Science/' +\n",
    "                            'spec1d*.fits')\n",
    "    target_files = []\n",
    "    for i in range(len(spec1d_list)):\n",
    "        if name in spec1d_list[i]:\n",
    "            target_files.append(spec1d_list[i])\n",
    "    return target_files\n",
    "\n",
    "def stack_1order(frame_list, order):\n",
    "    all_flux = []\n",
    "    all_ivar = []\n",
    "    all_wave = []\n",
    "    for frame_id in range(len(frame_list)):\n",
    "        hdul = fits.open(frame_list[frame_id])\n",
    "        data = hdul[order].data\n",
    "        all_flux.append(data['OPT_COUNTS'])\n",
    "        all_ivar.append(data['OPT_COUNTS_IVAR'])\n",
    "        all_wave.append(data['OPT_WAVE'])\n",
    "        hdul.close()\n",
    "    stacked_flux = np.median(all_flux, axis=0)\n",
    "    \n",
    "    stacked_var  = np.sum(div0(1,all_ivar),axis=0)/np.square(len(all_ivar[0]))\n",
    "    stacked_ivar = div0(1,stacked_var)\n",
    "    stacked_wave = np.median(all_wave, axis=0)\n",
    "\n",
    "    stacked_data = [stacked_wave, stacked_flux, stacked_ivar]\n",
    "    return stacked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b1def",
   "metadata": {},
   "source": [
    "### Read the model and sci data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1002189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:21:11.765685Z",
     "start_time": "2023-01-18T20:21:11.352433Z"
    }
   },
   "outputs": [],
   "source": [
    "# specifiy which date to work with\n",
    "date = '20180630'\n",
    "\n",
    "# read the model data\n",
    "A0_model = pd.read_csv('NIRES_PypeIt/A0V.dat',\n",
    "                       skiprows=2,\n",
    "                       header=None,\n",
    "                       names=['lk', 'ukf_a0v', 'uks_a0v', 'fh', 'fl', 'fc'],\n",
    "                       sep='  ',\n",
    "                       engine='python')\n",
    "\n",
    "A0_flux = df2array(A0_model[['ukf_a0v']])\n",
    "A0_wave = df2array(A0_model[['lk']])\n",
    "\n",
    "sci_files = find_files(date, 'J2202-0033')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca5a17",
   "metadata": {},
   "source": [
    "### Preform Telluric Correction Pre Slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ac5ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:21:50.819250Z",
     "start_time": "2023-01-18T20:21:50.793456Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the telluric data and airmass\n",
    "tell_files = find_files(date, 'HIP')\n",
    "\n",
    "tell_airmass_list = []\n",
    "for frame_id in range(len(tell_files)):\n",
    "    hdul = fits.open(tell_files[frame_id])\n",
    "    hdr = hdul[0].header\n",
    "    tell_airmass_list.append(hdr['AIRMASS'])\n",
    "    hdul.close()\n",
    "tell_airmass_list = np.array(tell_airmass_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03613778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:21:51.912026Z",
     "start_time": "2023-01-18T20:21:51.892921Z"
    }
   },
   "outputs": [],
   "source": [
    "def tellcorr_preslit(sci_file, peakcoord_offset=4, rel_height=0.85):\n",
    "    peak_coords = [9546, 10050, 10940, 12685, 12820]\n",
    "    sci_hdul = fits.open(sci_file)\n",
    "    sci_airmass = sci_hdul[0].header['AIRMASS']\n",
    "\n",
    "    # caculate the airmass difference\n",
    "    airmass_diff = tell_airmass_list - sci_airmass\n",
    "\n",
    "    # determine which telluric frame(s) to use by airmass\n",
    "    # if the differences are all positive, i.e. all tell airmass are\n",
    "    # greater than the sci airmass\n",
    "    if min(airmass_diff >= 0):\n",
    "        tell1_id = airmass_diff.argmin()\n",
    "        # path or filename\n",
    "        tell1_path = tell_files[tell1_id]\n",
    "        tell2_path = 'None'\n",
    "    # if all negative, i.e. all tell airmass smaller than the sci airmass\n",
    "    elif max(airmass_diff < 0):\n",
    "        tell1_id = airmass_diff.argmax()\n",
    "        # path or filename\n",
    "        tell1_path = tell_files[tell1_id]\n",
    "        tell2_path = 'None'\n",
    "    # when sci airmass is in between two tellairmass\n",
    "    else:\n",
    "        # tell1 is the frame with greater airmass\n",
    "        # tell2 is the frame with smaller airmass\n",
    "        tell1_id = airmass_diff.index(min(i for i in airmass_diff if i > 0))\n",
    "        tell2_id = airmass_diff.index(min(i for i in airmass_diff if i < 0))\n",
    "        tell1_airmass = tell_airmass_list[tell1_id]\n",
    "        tell2_airmass = tell_airmass_list[tell2_id]\n",
    "        # weight of the two telluric frames based on\n",
    "        # airmass relative to science airmass\n",
    "        weight1 = abs(tell2_airmass - sci_airmass)\n",
    "        weight2 = abs(tell1_airmass - sci_airmass)\n",
    "        # path or file name\n",
    "        tell1_path = tell_files[tell1_id]\n",
    "        tell2_path = tell_files[tell2_id]\n",
    "\n",
    "    ############################################################################\n",
    "    # start the mean process\n",
    "    for order in range(1, 6):\n",
    "        # define the science and telluric wave and flux\n",
    "        sci_data = sci_hdul[order].data\n",
    "\n",
    "        sci_wave, sci_flux = sci_data['OPT_WAVE'], sci_data['OPT_COUNTS']\n",
    "        sci_ivar = sci_data['OPT_COUNTS_IVAR']\n",
    "        sci_sig = sci_data['OPT_COUNTS_SIG']\n",
    "\n",
    "        # if the airmass of the sci frame is not in between two telluric arimasses,\n",
    "        # then just use one telluric\n",
    "        if tell2_path == 'None':\n",
    "            tell_hdul = fits.open(tell1_path)\n",
    "            tell_data = tell_hdul[order].data\n",
    "            tell_wave, tell_flux = tell_data['OPT_WAVE'], tell_data[\n",
    "                'OPT_COUNTS']\n",
    "            tell_hdul.close()\n",
    "            # resample telluric data to science data\n",
    "            tell_flux = resampling(sci_wave, tell_wave, tell_flux)\n",
    "\n",
    "        # if the airmass of the sci frame is in between two telluric arimasses,\n",
    "        # then caculate the weighted mean of the two telluric flux\n",
    "        else:\n",
    "            tell1_hdul = fits.open(tell1_path)\n",
    "            tell2_hdul = fits.open(tell2_path)\n",
    "            tell1_wave, tell1_flux = tell1_hdul[order].data[\n",
    "                'OPT_WAVE'], tell1_hdul[order].data['OPT_COUNTS']\n",
    "            tell2_wave, tell2_flux = tell2_hdul[order].data[\n",
    "                'OPT_WAVE'], tell2_hdul[order].data['OPT_COUNTS']\n",
    "            tell1_hdul.close()\n",
    "            tell2_hdul.close()\n",
    "            # resample telluric data to science data\n",
    "            tell1_flux = resampling(sci_wave, tell1_wave, tell1_flux)\n",
    "            tell2_flux = resampling(sci_wave, tell2_wave, tell2_flux)\n",
    "            tell_flux = (tell1_flux * weight1 +\n",
    "                         tell2_flux * weight2) / (weight1 + weight2)\n",
    "\n",
    "        # slice model wavelength to make it in the same range\n",
    "        # as the wavelenghth range of the working order\n",
    "        A0_lo = np.abs(A0_wave - sci_wave[0]).argmin()\n",
    "        A0_up = np.abs(A0_wave - sci_wave[-1]).argmin()\n",
    "        A0_wave_sliced = A0_wave[A0_lo:A0_up]\n",
    "        A0_flux_sliced = A0_flux[A0_lo:A0_up]\n",
    "        # resample the model data to the same rate as the science data\n",
    "        A0_flux_sliced = resampling(\n",
    "            sci_wave,\n",
    "            A0_wave_sliced,\n",
    "            A0_flux_sliced,\n",
    "        )\n",
    "\n",
    "        # define the telluric correction factor\n",
    "        tellcorr_factor = div0(A0_flux_sliced, tell_flux)\n",
    "\n",
    "        # interpolate the pachen series peaks\n",
    "        for peak_coord in peak_coords:\n",
    "            # if the wavelength is in the range of the order\n",
    "            if peak_coord >= sci_wave[0] and peak_coord <= sci_wave[-1]:\n",
    "                # find the x index of the peak\n",
    "                # there might be an offset so check the near values to find the real peak\n",
    "                # the offset might be changed\n",
    "                given_peak_id = abs(peak_coord - sci_wave).argmin()\n",
    "                peak_range_id = np.linspace(given_peak_id - peakcoord_offset,\n",
    "                                            given_peak_id + peakcoord_offset,\n",
    "                                            2 * peakcoord_offset + 1,\n",
    "                                            dtype=int)\n",
    "                peak_range_values = []\n",
    "                for i in peak_range_id:\n",
    "                    peak_range_values.append(tellcorr_factor[i])\n",
    "                # use the real peak value to find the index\n",
    "                peak_id = [peak_range_id[np.array(peak_range_values).argmax()]]\n",
    "                peak_width = sp.peak_widths(tellcorr_factor, peak_id,\n",
    "                                            rel_height)[0]\n",
    "                # find the index of the right and left limit of the peak\n",
    "                # assume symmetry\n",
    "                right_id = peak_id[0] + round(peak_width[0] / 2)\n",
    "                left_id = peak_id[0] - round(peak_width[0] / 2)\n",
    "                width = right_id - left_id\n",
    "                # the left and right x indices around the peak,\n",
    "                # used to interpolate new values for the peak region\n",
    "                xp = np.hstack((np.linspace(left_id - width,\n",
    "                                            left_id - 1,\n",
    "                                            width,\n",
    "                                            dtype=int),\n",
    "                                np.linspace(right_id + 1,\n",
    "                                            right_id + width,\n",
    "                                            width,\n",
    "                                            dtype=int)))\n",
    "                # the corresponding y values\n",
    "                fp = []\n",
    "                for i in xp:\n",
    "                    fp.append(tellcorr_factor[i])\n",
    "                # interpolate range (the width of the peak)\n",
    "                x_interp = np.linspace(left_id, right_id, width + 1, dtype=int)\n",
    "                # interpolate\n",
    "                interp_y = np.interp(x_interp, xp, fp)\n",
    "                # rewrite and replace\n",
    "                for i in range(len(x_interp)):\n",
    "                    tellcorr_factor[x_interp[i]] = interp_y[i]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # perform the correction\n",
    "        tellcorr_sci = np.multiply(sci_flux, tellcorr_factor)\n",
    "\n",
    "        # rescale (not propograte) the error\n",
    "        tellcorr_sci_sig = np.multiply(sci_sig, div0(A0_flux_sliced,\n",
    "                                                     tell_flux))\n",
    "\n",
    "        # save the result to over write the origional data\n",
    "        sci_hdul[order].data['OPT_COUNTS'] = tellcorr_sci\n",
    "        sci_hdul[order].data['OPT_COUNTS_IVAR'] = div0(\n",
    "            1, np.square(tellcorr_sci_sig))\n",
    "        sci_hdul[order].data['OPT_COUNTS_SIG'] = tellcorr_sci_sig\n",
    "\n",
    "    return sci_hdul.writeto('NIRES_PypeIt/' + date + '_reduced/' +\n",
    "                            'tellcorr_science/' + sci_file[38:44] +\n",
    "                            '_tellcorr' + sci_file[44:],\n",
    "                            overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6b9ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T20:21:59.052158Z",
     "start_time": "2023-01-18T20:21:54.311759Z"
    }
   },
   "outputs": [],
   "source": [
    "for sci_file in sci_files:\n",
    "    tellcorr_preslit(sci_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dde29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
